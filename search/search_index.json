{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Introduction","text":"<p>This site describes the inner workings of arxiv.org \u2013 the software engineering, operational processes, etc., that go into keep arxiv.org running and performing its mission day after day.</p> <p>This is not the place to find information about how to use arXiv. For information about submitting papers, reading papers, formating, etc, please refer to the arXiv help pages. </p> <p>The primary, but not exclusive focus of this site is to document the technical revamp of arXiv that's we're calling arXiv CE (Cloud Edition). As the name implies, this is the project to host arXiv in the Cloud. Read more about that on the arXIv CE page.</p> <p>As an experiment, we've inviting community participation in this effort.  See Open For public Discussion for the details.</p>"},{"location":"Public_Forum.html","title":"Public Forum","text":""},{"location":"Public_Forum.html#open-for-public-discussion","title":"Open for Public Discussion!","text":"<p>With the arXiv CE project, we'll be making a lot of changes to the way arXiv works. We believe that the wider arXiv community contains technical expertise that can be useful in reviewing and discussion our designs. To that end, we've set up this site to allow the arXiv community to enter comments \u2013 which will be publicly visible \u2013 on each page.</p> <p>Some ground rules for commenting:</p> <ul> <li>All participants must agree to adibe by the overall arXiv Code of Conduct</li> <li>Please don't just chime in with unsupported opinions, but rather:<ul> <li>provide references to papers or articles that support your suggestions (bonus points for papers on arXiv \ud83d\ude42).</li> <li>or provided detailed descriptions based on your own actual expeirence with similar projects/technologies</li> </ul> </li> <li>We welcome spirited discussions, but please don't resort to personal attacks.</li> <li>We research the right to moderate posts, and even ban user if things get out of hand. We're hoping we won't ever have to this.</li> </ul> <p>The technology used:</p> <ul> <li>The source for the content on this site is hosted on GitHub, and mostly written in Markdown (with occaisional raw HTML)</li> <li>We use mkdocs to generate static HTML pages from markdown source via a GitHub Action</li> <li>The discusson comments are in GitHub Discussions. The \"Comments\" section on each page is enabled through a clever client-side Javacript application called: giscus<ul> <li>If you don't care to comment on the page, or you don't have Javascript enabled, you can still participate in the discusion by going directly to GitHub Discussions</li> </ul> </li> </ul>"},{"location":"arXiv_BC_%28Legacy%29/index.html","title":"arXiv-BC","text":"<p>\"BC\" is \"Before Cloud\". This section is a description of arXiv as it existed circa 2022, which is before most services started being migratated to the cloud. This represents a snapshot in time that's described here mostly as a comparison point \u2013 which can be used as reference point for the changes we're making and plan to make.</p> <p> A description of arXiv circa 2022 <p></p></p> <p> Issues with arxiv.org circa 2022</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html","title":"arXiv Legacy System","text":"<p>This page describes the state of arXiv as it has generally been from 2010 through the start of 2023.</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#software-components","title":"Software components","text":"<p>The major software components of arXiv are illustrated in the simplified schematic below and described in the following sections.</p> <p></p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#browse-search-download","title":"Browse, Search, Download","text":"<p>The vast majority of arXiv users are \"read-only\". THey view abstracts, download articles, and search for articles. The use of arXiv's own search has been descreasing over time, as more users now seem to access articles found through sources like Google Scholar than arXiv's native search (by a small plurality). But that's not really relevant in terms of examining arXiv's own implementation structure, as there's no direct tie-in with the outside search facilities.</p> <p>Most (all?) of the read-only portion of arXiv is modern, in the sense that Python is the implmentation language, as opposed to the Perl (and some PHP) implementations of the older parts of arXiv. The UI pages are bult using the Python Flask micro-framework.</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#home-page","title":"Home Page","text":"<p>The arXiv \"home page\" --  lists all the categories that arXiv presently supports, a search box, and little else aside from links to more information about arXiv in the headers and footers.</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#submisson-system","title":"Submisson system","text":"<p>The arXiv submission system accepts uploads from individual arXiv users (along with metadata, classification information, and license data) and arranges it for moderation and publication. The system is designed so that without intervention from either admins or moderators, all submissions made before a certain freeze time (currently 2pm Mon\u2013Fri) will be made public at the next publication time (currently 8pm Sun\u2013Thu, i.e. a 6h gap between freeze and publish except Fri to Sun). The submission system was rewritten in 2010 using the Perl/Catalyst framework.</p> <p>In addition to the submission interface for users, there is a machine interface that implements the SWORD protocol (v1.3 with arXiv customizations). The SWORD interface is intended to facilitate bulk upload of conference proceedings, remote submission sites (notably CCSD, see https://hal.archives-ouvertes.fr/page/transfert-vers-arxiv), journal migrations, etc., via registered and trusted proxy submitters. The interface is documented in the public manual at http://arxiv.org/help/submit_sword, that also includes example code to use the interface.</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#administration-and-moderation","title":"Administration and moderation","text":"<p>The arXiv system has special user interfaces for moderators and administrators. (If you're not familiar with arXiv's moderation, see https://info.arxiv.org/help/moderation/index.html) for background). There are actually three generations </p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#hardware-infrastructure","title":"Hardware infrastructure","text":"<p>Since the LANL preprint archive moved to Cornell in 2001 and became arxiv.org, nearly all arXiv services have been hosted in infrastructure owned and managed by the Cornell Information Technology (CIT) organiziation.</p> <p>Below is an aproximate diagram of the arXiv server farm, circa 2017. The general shape today, at the start of 2023 is about the same, but some servers, like web1, web2, web3, web4, have been retired (because the OSes they were running went out of support), and have been replaced by a new set of web servers like web7, web8, web9, and web10. </p> <p>Note the the servers starting \"CUL\" and \"CIT\" belong to CIT and perform the listed services on behalf of all CIT customers, not just arXiv. CIT runs a fairly large hosting infrastructure for a University. arXiv may be one of CIT's largest \"customers\", but constitutes only a few percent (by count) of the servers that CIT runs.</p>"},{"location":"arXiv_BC_%28Legacy%29/arXiv_Present_State.html#server-configuruations","title":"Server configuruations","text":"<p>Configuration of each web node: </p>"},{"location":"arXiv_BC_%28Legacy%29/legacy.html","title":"Legacy Issues","text":"<p>As might be expected for a 30+ year old system, arXiv suffers from a number of legacy issues.</p>"},{"location":"arXiv_BC_%28Legacy%29/legacy.html#growing-requirements","title":"growing requirements","text":"<ul> <li>arXiv usage is growing</li> <li>heading from about 150k per year to 00k+</li> <li>lots of delayed feature requests</li> <li>Example: research agencies want funding meta data attached to articles</li> <li>accessibility requirements</li> </ul>"},{"location":"arXiv_BC_%28Legacy%29/legacy.html#inflexible-infrastructure","title":"Inflexible infrastructure","text":"<ul> <li>services run directly on VMs</li> <li>a new VM takes several days to configure</li> <li>configuration process is a manual based on a ~30 item checklist</li> <li>we know there are differences between our VM nodes, and fixing this is hard</li> <li>no comprehensive automated test suite to ensure VM is completely functional</li> <li>CentOS 7, which is currently deployed on nearly all VMs will be EOL in a year</li> <li>we're running an unsupported version of Apache, because the later versions would conflict with some of the legacy packages we use</li> </ul>"},{"location":"arXiv_BC_%28Legacy%29/legacy.html#inadequate-secops","title":"inadequate secOps","text":"<ul> <li>deploys are largely manual, and occur one at a time for each web node</li> <li>no comphrehensive test suites</li> <li>very few tests at all</li> <li>too many git repos, hard to deploy a set of cooredinated components atomically</li> <li>monitoring and alerting facilities are limited</li> </ul>"},{"location":"arXiv_BC_%28Legacy%29/legacy.html#aging-code-base","title":"Aging code base","text":"<ul> <li>The submission process is almost all legacy perl code</li> <li>Hard to find perl programmers</li> <li>We don't want to continue writing new perl code</li> <li>The user management system is legacy PHP code</li> <li>based on a PHP version that's no longer supported</li> <li>breaking changes in new version of PHP make upgrading problematic</li> <li>highly problematic TeX/LaTeX pipeline</li> <li>a number of old articles no longer build in the automated process</li> <li>we fell two years behind in the version of TeX we use for new articles</li> <li>a LaTeX article that shows correctly in Overleaf might now compile on arXiv</li> </ul>"},{"location":"arXiv_CE_%28Cloud_Edition%29/index.html","title":"arXiv-CE","text":"<p>CE stands for Cloud Edition.</p> <p>This is the public documentation site for the project to move arXiv to the cloud. We will use this place to share our plans with the arXiv community, and also seek feedback and suggestions from the community. We will try to keep this site up to date as our plans become implementations and as our implementations are put into production.</p> <p>We're inviting community participation in this effort. See Open For public Discussion </p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/index.html#why-is-arxiv-moving-to-the-cloud","title":"Why is arXiv moving to the cloud?","text":"<p>First, see the legacy issues described here. </p> <p>What moving to the cloud will enable:</p> <ol> <li>Flexible, real-time scaling nearly without limits to keep up with our growth</li> <li>A more modern development devops environment<ul> <li>deployment via Docker images</li> <li>Kubernetes like infrastructure for bulk deployment with no downtime</li> <li>better integrated logging and monitoring</li> <li>support for integrated CI/CD suites</li> </ul> </li> <li>Better international support<ul> <li>Cornell has excellent bandwidth in North America, but download times are poor overseas</li> </ul> </li> <li>Greater reliability<ul> <li>Cornell IT has, in general, provided exemplary uptime over the last 15 years</li> <li>But we have, on a couple of occasions, experienced downtime due to Cornell IT actions</li> <li>There is no geogrpahic diversity<ul> <li>We are dependent on a set of servers housed in a single facility</li> <li>Restoring services if that facility were to become unavailable for an extended period of time would be a monumental effort</li> </ul> </li> <li>The major cloud vendors all have geographically diverse datacenters<ul> <li>unlikely that more than one would fail simultaneously</li> </ul> </li> </ul> </li> </ol>"},{"location":"arXiv_CE_%28Cloud_Edition%29/plan.html","title":"Overview of arXiv CE plan","text":"<p>We will be using the Google Cloud (GCP). This is mostly because Google has been generous with Cloud credits. We fully intend to rebuild arXiv to use as many cloud-centric services (more on that below), with no thought given to ever running on a legacy dedicated VM infrastructure ever again.</p> <p>Won't this lock us into Google as vendor? Yes, to some extent, but not as much as it might seem. There a good comparison chart of of the services offered by GCP, Azure, and AWS here: https://www.techtarget.com/searchcloudcomputing/feature/A-cloud-services-cheat-sheet-for-AWS-Azure-and-Google-Cloud. The overall conclusion is that nearly all services offered by one vendor are also offered by the other two. We'e away that there can be differences in how each service is configured, and the APIs might be quite different. But we believe that the similarities are strong enough that it is feasible to port a complex system, such as arXiv from one to another at a cost of between 10 and 15% of the effort to create the original system. And particular, the service we expect to depend most heaviy on -- Google Cloud Run, has counterparts in each of the other Cloud vendors:</p> AWS Azure Google Cloud Container registry Amazon Elastic Container Registry (ECR), ECR Public Azure Container Registry Artifact Registry, Container Registry Managed container service AWS Copilot, Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS) Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) Serverless containers AWS App Runner, AWS Fargate Azure Container Instances (ACI) Cloud Run <p>## Implementation principles</p> <ul> <li>All code will be in Python<ul> <li>We do not intend to port any of the legacy perl or PHP code to the cloud</li> </ul> </li> <li>Choose a coding standards and styles, and stick to it<ul> <li>One candidate: https://peps.python.org/pep-0008/</li> <li>Another candidate: https://peps.python.org/pep-0008/</li> <li>Pick a dependency/package management strategy and stick to it</li> </ul> </li> <li>Insofar as possible, all arXiv code will be packaged in containers and deployed using Cloud Run<ul> <li>Except, maybe, for services that need large amounts of persistent in-memory data</li> <li>Except, maybe, for some very simple, largely-applicative Cloud Functions</li> <li>Except for managed services (see below)</li> </ul> </li> </ul>"},{"location":"arXiv_CE_%28Cloud_Edition%29/plan.html#managed-services","title":"Managed services","text":"<ul> <li>In order make best use of our limited staff, we want to use managed services as much as possible</li> <li>To keep administrative overhead down, use services from as few vendors as possible</li> <li>If our primary Cloud vendor (GCP) has a service that meets our minimum requirements, use it, even if another vendor might have a slightly superior solution</li> <li>Managed service we expect to use:<ul> <li>GCP Cloud SQL (mySQL and/postgresql)</li> <li>GCP Cloud Run (so we don't have to manage our own Kubernetes Cluster)</li> <li>GCP CDN</li> <li>GCP Cloud Logging</li> <li>GCP Cloud Monitoring Monitoring</li> <li>GCP Cloud Armor for:<ul> <li>Intrusion detection</li> <li>Protection against DOS attacks, and/or clients with excessively high load</li> </ul> </li> <li>GCP Pub/Sub and/or Cloud Tasks</li> <li>Atlassion OpsGenie for alerts<ul> <li>Google doesn't have a serious comparable service to handle on-call schedules and notifications</li> <li>We're already using OpsGenie and other Atlassian projects, so it doesn't add another vendor the mix</li> <li>Even though PagerDuty has a superior service \ud83d\ude00 </li> </ul> </li> </ul> </li> </ul>"},{"location":"arXiv_CE_%28Cloud_Edition%29/plan.html#opsec","title":"OpSec","text":"<ul> <li>The system will be build using Infrastucture as Code principles -- the declarative version</li> <li>The entire state of the system, as deployed, shall reside in Git<ul> <li>No production services shall be deployed or configured manually (this is aspirational right now)</li> <li>Git provides many essential features for deployment management:<ul> <li>Change control (pull requests with an approval process)</li> <li>Audit trail</li> <li>Easy roll-back</li> </ul> </li> <li>Seperation of source code -- programs that implements arxiv.org, from the configuraiton that deploys arxiv.org<ul> <li>Makes it easier and cleaner to have beta, dev, test, etc. deployments</li> <li>Makes it easier to re-use modules, perhaps for other arXiv-like services</li> <li>Forsees other deployment targets -- potentially AWS or Azure, simply by having different deployment configutations (this only works if our code handles API differences)</li> </ul> </li> </ul> </li> </ul>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/index.html","title":"Browsing and Searching on arXiv","text":"<p>Introductory content to this subsection. We need an index.md as a landing page.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html","title":"Proposal for PDF CDN","text":"<p>This is a proposal for how to serve PDFs from Google's CDN. This proposal will cover how to arrange the PDF files in Google storage, the load balancer, any services and the CDN to support the goals.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#goals","title":"Goals","text":"<ol> <li>Favor using Google products over writing code</li> <li>The PDF for the current version is the hot path</li> <li>Avoid redundant copies of the PDFs in GS</li> <li>Serve current version at <code>https://arxiv.org/pdf/2001.00021.pdf</code></li> <li>Serve other versions at <code>https://arxiv.org/pdf/2001.00021v3.pdf</code></li> <li>Maintain long standing URL patterns</li> <li>Not excessive time to download PDFs</li> </ol> <p>The only goals we can relax are 1. favor google products, 3. avoid redundant copies. We could relax 6 maintain URL patterns but that is probably a policy decision. We could relax 7 but it doesn't seem too relevant.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#considerations","title":"Considerations","text":""},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#current-version","title":"Current version","text":"<p>There are two main kinds of URLs we need to handle, the current version URLs and the versioned URLs. The current version URLs have filenames like <code>1234.12345.pdf</code> that lack the vN version number. These are expected to provide the user with the most current version of that paper. This can return different content at different times due to revisions to the paper. These revisions are not more than once a day. The versioned have a vN like <code>1234.12345v3.pdf</code> and are expected to return the PDF for that version. These are expected to almost never change.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#traffic","title":"Traffic","text":"<p>Looking at a 1h window on 2022-10-28. Successful PDF requests with version number: 3766 Successful PDF requests without version number: 42421.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#throughput","title":"Throughput","text":"<p>427000 bytes/s for successful PDFs over a 1h window on 2022-10-28.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#revisions","title":"Revisions","text":"<p>Over the whole corpus about 50% of papers have revisions. About 8% of the articles submitted in 2022-10 had versions during the same month.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#out-of-band-changes","title":"Out of band changes","text":"<p>There are infrequent changes made outside of the versioning system. Jim says they do about 1 or 2 a week. He said that almost all of these go through set of utilities that schedule the changes to the mirroring system. The admins don't login into the Cornell VMs and make changes, they use a Perl script to rsync changes to their laptop, make the changes and then use a script to rsync the changes back to the VM and schedule the sync request for the mirrors. </p> <p>See: confluence: Hacking Submissions and confluence Admin set-up for a local admin machine</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#persistent-url-and-link-rot","title":"Persistent URL and link rot","text":"<p>arXiv has a history maintaining URLs for the PDFs and abs pages. For older PDFs:</p> <pre><code>https://arxiv.org/pdf/astro-ph/0606003.pdf\nhttps://arxiv.org/pdf/astro-ph/0606003v99.pdf\nhttps://arxiv.org/pdf/astro-ph/0606003\nhttps://arxiv.org/pdf/astro-ph/0606003v99\n</code></pre> <p>For newer:</p> <pre><code>https://arxiv.org/pdf/2001.00021 \nhttps://arxiv.org/pdf/2001.00021v99\nhttps://arxiv.org/pdf/2001.00021.pdf \nhttps://arxiv.org/pdf/2001.00021v99.pdf\n</code></pre> <p>It has been the practice at arxiv.org to do an HTTP redirect from <code>https://arxiv.org/pdf/2001.00021</code> to <code>https://arxiv.org/pdf/2001.00021.pdf</code> in order to have a reasonably named download. There are also redirects from HTTP to HTTPS.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#limitations-to-gcp-lb-rules","title":"Limitations to GCP LB rules","text":"<p>The GCP LB has URL mapping and rules but they are limited. \"Path matchers (and URL maps in general) do not offer features that function like Apache LocationMatch directives. If you have an application that generates dynamic URL paths that have a common prefix, such as /videos/hd-abcd and /videos/hd-pqrs, and you need to send requests made to those paths to different backend services, you might not be able to do that with a URL map. For simple cases containing only a few possible dynamic URLs, you might be able to create a path matcher with a limited set of path rules. For more complex cases, you need to do path-based regular expression matching on your backends.\" see https://cloud.google.com/load-balancing/docs/url-map-concepts#wildcards-regx-dynamic</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#no-custom-404-handling-in-lb","title":"No custom 404 handling in LB","text":"<p>I also was looking to see if we could route all 404s to a service but didn't find anything like that. This issue indicates that this is not a feature: Allow Cloud Storage users to specify an http status code for 404 pages</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#cdn-cache-keys","title":"CDN Cache keys","text":"<p>https://cloud.google.com/cdn/docs/caching#cache-keys GCP can include headers and cookies as part of the cache key. Some of the GS query parameters are set as default parts of the cache key when the back end is a storage bucket.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#gs-buckets-versions","title":"GS Buckets versions","text":"<p>CDN can cache based on query parameter <code>?generation=1234</code>. It sounds like we can alter earlier versions. The generations can be listed. https://cloud.google.com/storage/docs/object-versioning</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#no-symbolic-links-in-gs","title":"No symbolic links in GS","text":"<p>GS does not support symbolic links. \"...there are no plans for gsutil rsync to support operating system-specific file types like symlinks\" from the gsutil rsync docs</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#downloaded-files-end-with-pdf","title":"Downloaded files end with .pdf","text":"<p>We redirect to a URL that ends with <code>.pdf</code>. This is likely to make the downloaded file more useful since OSs will recognize it as a PDF and open it with the appropriate viewer. We could achieve the correct filename with a Content-Disposition HTTP header.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#timing","title":"Timing","text":"<p>The normal path at arxiv.org to get a PDF uses a redirect. The abs pages have <code>arxiv.org/pdf/astro-ph/0606003</code> and that gets redirected to <code>arxiv.org/pdf/astro-ph/0606003.pdf</code>. </p> <p>A request for <code>arxiv.org/pdf/astro-ph/0606003</code> is: 58ms for first redirect, 655ms for the 2nd request that does the download.</p> <p>Google serves <code>astro-ph/0606003v1.pdf</code>: 218ms wait for response, 51ms download. This is going directly without any redirects.</p> <p>These times omit DNS and SSL. These don't need to be top level concerns but we do expect users to come without keep alive from email clients. To arxiv.org I'm seeing these as around DNS 54ms, initial connection+ssl: 135ms.  To google I'm seeing 50ms, initial connection+ssl: 95ms.</p> <p>These times are not averages, they are for single requests.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#proposal-1","title":"Proposal #1","text":"<p>It seems the goal of no-code conflicts with goals of no redundancy and current version. In discussing this Nov. 2 2022 Charles F. suggested we relax the redundancy goal.</p> <p>The mappings look like:</p> <pre><code>Served URL                                GS URL\nhttps//arxiv.org/pdfs/1111.22222.pdf   -&gt; gs://pdf-bucket/pdf/1111.22222.pdf\nhttps//arxiv.org/pdfs/1111.22222v1.pdf -&gt; gs://pdf-bucket/pdf/1111.22222v1.pdf\nhttps//arxiv.org/pdfs/1111.22222v2.pdf -&gt; gs://pdf-bucket/pdf/1111.22222v2.pdf\n</code></pre> <p>And in the case of v2 being the most recent version, 1111.22222.pdf would be the same content as 1111.22222v2.pdf</p> <ol> <li>PDFs in GS arranged with the keys in a way that allows them to be    served as a web site with the expected URL paths. Redundent files    would be stored if the same content needed to be served from    different paths.</li> <li>The bucket these are in is set as \"serve as public website\"</li> <li>A LB and CDN is setup to serve these at some domain name.</li> <li>A sync service would copy the content to the correct GS bucket    locations at announce time.</li> </ol> <p>I don't see an API that says <code>put_content_at_CDN_key(key, content)</code>. We can think of a service as that API. The service gets a request from the CDN and then it can be programmed to put any content there.</p> <p>Brian Caruso initially suggested we reject this proposal. A small app the mediates the URL to object mapping is a large gain of flexibility for little additional complexity. The state of the objects in the GS becomes much simpler.</p> <ol> <li>The goal of no-code, GCP only-products forces limitations on what    URL to file mappings are possible due to the limitations of the GCP    products.</li> <li>We would end up with a system where the state of the bucket was    constantly changing to support the URL mapping. The storage and the    keys used should be limited to the concerns of storage and not have    to also carry the concerns of URL mapping.</li> <li>It would be easier to test that an app returns the correct file for    a URL than to test CDN/LB/GS/obj-naming-conventions.</li> </ol>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#migration-proposal","title":"Migration proposal","text":"<p>This is a proposal to use the existing <code>arxiv.org</code> infrastructure at Cornell but serve the PDFs from a CDN as an intermediate step towards an all on cloud stance.</p> <ol> <li>Implement the one of the proposals but have it at a host name like    <code>cloud.arxiv.org</code></li> <li>At announce time, build PDFs and sync to Google</li> <li>Add <code>/pdf</code> path to arxiv-browse that uses this logic<ol> <li>Check if CDN is fresh</li> <li>yes? HTTP redirect to CDN</li> <li>no? reverse proxy the legacy PDF endpoint</li> </ol> </li> </ol> <p>The check of CDN freshness could be cached. The reverse proxy avoids an unnecessary HTTP redirect on the non-fresh path.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#proposal-2-pdf-service","title":"Proposal #2 PDF service","text":"<p>This is a proposal for what would be ideal without any legacy concerns other than URL patterns.</p> <ol> <li>PDFs in GS with version numbers</li> <li>arxiv.org presents URLs to PDFs like https://arxiv.org/pdf/1234.12345.pdf</li> <li>The <code>/pdf</code> path URL mapped at the LB a backend with a CDN <code>pdf-service</code> an app running on Cloud Run</li> <li>That service goes to the GS and streams the content back to the client</li> <li>Cache warming for new PDFs as part of the announce process</li> </ol> <p>This meets all goals except for 1 no-code. This proposal was rejected due to the decision to relax the redundency goal and that we could move from proposal #1 to #2 in the future if needed.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/pdf-cdn.html#caching-cdn-freshness","title":"Caching CDN Freshness","text":"<p>The CDN freshness will be checked from Cornell by doing a HEAD request to the CDN and comparing the <code>last-modified</code> time with the mtime of the Cornell PDF and the mtime of the Cornell PDF with the mtime of the source.</p> <p>One question that came up is about the CDN/GS <code>last-modified</code> time. Is it the mtime of the original PDF at Cornell or the time the sync happened? We can avoid this question by running <code>gcloud rsync -P</code> which will preserve the access/modification timestamps.</p> <p>But even then are there problems?</p> <p>Yes, once a HEAD request is done from Cornell, there is a race condition between the client download and the possibility of a change of the PDF. The HEAD then HTTP Redirect transfers the happens first ordering data about <code>PDF_change-&gt;PDF_sync</code> to the client. But in the PDF at Cornell could change creating a new ordering.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/test1.html","title":"Test page 1","text":"<p>more content for testing purpoes only.</p> <pre><code>code here code here\n{ testing }\n\n</code></pre>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/test2.html","title":"Test page 2","text":"<p>more content for testing purposes only.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/test2.html#this-is-a-subhead","title":"this is a subhead","text":"<p>some text here.</p>"},{"location":"arXiv_CE_%28Cloud_Edition%29/Browsing_%26_Searching/test2.html#third-level-header","title":"third level header","text":"<p>Text goes here. Written content. Text goes here. Written content.</p> <pre><code>code here code here\n{ testing }\n\n</code></pre>"}]}